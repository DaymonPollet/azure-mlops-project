name: Azure ML Job Pipeline # Display name for your workflow in GitHub Actions
on:
  workflow_dispatch: # Allows you to manually trigger the workflow from GitHub UI
  # Uncomment the following block later if you want the workflow to run on every push to 'main'
  # push:
  #   branches:
  #     - main

env:
  GROUP: mlops-demo 
  WORKSPACE: pollet-daymon-ml 
  LOCATION: westeurope 

jobs:
  azure-pipeline:
    runs-on: ubuntu-24.04 
    steps:
      # 1. Check out code repository: Clones your GitHub repository into the runner
      - name: Check out code repository
        uses: actions/checkout@v4

      # 2. Azure login: Logs into Azure using the Service Principal credentials stored in GitHub Secrets
      - name: Azure login
        uses: azure/login@v2
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      # 3. Azure test - Get Compute: (Demonstration step)
      #    Installs Azure ML CLI extension and lists computes to verify connection
      #    This step is directly from your assignment instructions.
      - name: Azure test - Get Compute
        uses: azure/CLI@v2.1.0
        with:
          azcliversion: latest # CHANGED THIS LINE
          inlineScript: |
            az extension add --name ml
            az configure --defaults group=$GROUP workspace=$WORKSPACE location=$LOCATION
            az ml compute list
            
      # 4. Azure -- Component Setup (from assignment)
      # Reusing 'latest' for azcliversion
      - name: Azure -- Component Setup
        uses: Azure/CLI@v2.1.0
        with:
          azcliversion: latest
          inlineScript: |
            az extension add --name ml
            az configure --defaults group=$GROUP workspace=$WORKSPACE location=$LOCATION
            az ml component create --file ./components/dataprep/dataprep.yaml # Corrected from components to component, if you get an error revert to components
            az ml component create --file ./components/dataprep/data_split.yaml # Corrected from components to component
            az ml component create --file ./components/training/training.yaml # Corrected from components to component

      # 5. Pipeline in Azure - Start the training job (from assignment)
      # Reusing 'latest' for azcliversion
      - name: Azure -- Start Training Job
        uses: Azure/CLI@v2.1.0
        with:
          azcliversion: latest
          inlineScript: |
            az extension add --name ml
            az configure --defaults group=$GROUP workspace=$WORKSPACE location=$LOCATION
            az ml job create --file ./pipeline.yaml --stream --set name=animals-classification-${{ github.sha }}-${{ github.run_id }}

      # 6. Stop the compute machine (from assignment)
      # Reusing 'latest' for azcliversion and adding continue-on-error
      - name: Azure -- Stop Compute
        uses: Azure/CLI@v2.1.0
        with:
          azcliversion: latest
          inlineScript: |
            az extension add --name ml -y # Added -y for non-interactive
            az configure --defaults group=$GROUP workspace=$WORKSPACE location=$LOCATION
            az ml compute stop --name cli-created-machine
        continue-on-error: true # Ignore errors if compute is already stopped

  download:
      needs: azure-pipeline # New!! This job will only run after 'azure-pipeline' succeeds
      runs-on: ubuntu-24.04
      steps:

      - name: Check out repository
        uses: actions/checkout@v4

      - name: Azure Login
        uses: azure/login@v2
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Azure -- Download Model
        uses: azure/CLI@v2.1.0
        with:
          azcliversion: latest # Use 'latest' as it works for you
          inlineScript: |
            az extension add --name ml -y
            az configure --defaults group=$GROUP workspace=$WORKSPACE location=$LOCATION
            # Get the latest version of the 'animal-classification' model
            VERSION=$(az ml model list -n animal-classification --query "[0].version" -o tsv)
            # Download the model. Assuming it should be downloaded to the current working directory.
            # If your model is in an asset (e.g. zip) you may need to specify the --output-path
            az ml model download --name animal-classification --version $VERSION

      # New!! Uploads the content of the 'inference' directory as an artifact
      - name: Docker -- Upload API code from Inference
        uses: actions/upload-artifact@v4.3.3
        with:
          name: docker-config
          path: inference

  deploy:
    needs: download
    runs-on: ubuntu-24.04 # Keeping ubuntu-24.04 for quick lab completion as discussed
    steps:
    - name: Check out repository
      uses: actions/checkout@v4

    - name: Docker -- Gather Tags
      id: docker-meta-data
      uses: docker/metadata-action@v5.5.1
      with:
        images: |
          ghcr.io/daymonpollet/mlops-animals-api # Ensure this is your GitHub username
        tags: |
          type=ref,event=branch
          type=sha
    
    - name: Docker -- Login to GHCR
      uses: docker/login-action@v3.2.0
      with:
        registry: ghcr.io
        username: ${{ github.repository_owner }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Docker -- Download API Code for Inference
      # This step downloads the 'inference' directory content.
      # Your Dockerfile (at the root) might then copy specific files from this 'inference' directory.
      uses: actions/download-artifact@v4.1.7
      with:
        name: docker-config
        path: inference

    - name: Docker Build and push
      id: docker_build
      uses: docker/build-push-action@v5.3.0
      with:
        context: . # <--- **FIXED: Dockerfile is in the repository root**
        push: true
        tags: ${{ steps.docker-meta-data.outputs.tags }}
        # If your Dockerfile has a different name (e.g., Dockerfile.dev), you would use: file: Dockerfile.dev

    - name: Deploy to Kubernetes
      uses: azure/CLI@v2.1.0
      with:
        azcliversion: latest
        inlineScript: |
          # This step assumes 'kubectl' is installed and configured on the runner.
          # The 'k3d-deployment.yaml' file is in the repository root.
          kubectl apply -f k3d-deployment.yaml # <--- **FIXED: Using your specified k3d manifest**