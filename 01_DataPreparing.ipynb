{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### First cells"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1748626154537
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "## Either get environment variables, or a fallback name, which is the second parameter.\n",
        "## Currently, fill in the fallback values. Later on, we will make sure to work with Environment values. So we're already preparing for it in here!\n",
        "workspace_name = os.environ.get('WORKSPACE', 'pollet-daymon-ml')\n",
        "subscription_id = os.environ.get('SUBSCRIPTION_ID', '6a36bb7a-aee5-4e15-a3c7-2e362d2c2387')\n",
        "resource_group = os.environ.get('RESOURCE_GROUP', 'mlops-demo')"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1748626154709
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The credential \"DefaultAzureCredential\" will use the same name as your logged in user.\n",
        "credential = DefaultAzureCredential()"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1748626154882
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client = MLClient(\n",
        "    credential, subscription_id, resource_group, workspace_name\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1748626156199
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Compute Machine from the SDK"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Instances need to have a unique name across the region.\n",
        "from azure.ai.ml.entities import ComputeInstance, AmlCompute\n",
        "\n",
        "ci_basic_name = \"cpu-daymon-auto\" # I add the suffix Auto, because we are automatically creating this instance.\n",
        "idle_shutdown_minutes = 30\n",
        "\n",
        "ci_basic = ComputeInstance(\n",
        "    name=ci_basic_name,\n",
        "    size=\"STANDARD_DS3_v2\",\n",
        "    idle_time_before_shutdown_minutes=idle_shutdown_minutes # by adding this line we can configure a shutdown if idle\n",
        ")\n",
        "\n",
        "print(f\"Creating or updating compute instance '{ci_basic_name}' with size '{ci_basic.size}' and idle shutdown after {idle_shutdown_minutes} minutes of inactivity...\")\n",
        "\n",
        "ml_client.begin_create_or_update(ci_basic).result()\n",
        "\n",
        "print(f\"Compute instance '{ci_basic_name}' created/updated successfully!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Creating or updating compute instance 'cpu-daymon-auto' with size 'STANDARD_DS3_v2' and idle shutdown after 30 minutes of inactivity...\nCompute instance 'cpu-daymon-auto' created/updated successfully!\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1748626189338
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pasting contents of the yaml file to the right directory using code."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory_path = \"components/dataprep\"\n",
        "\n",
        "file_path = os.path.join(directory_path, \"conda.yaml\")\n",
        "\n",
        "conda_yaml_content = \"\"\"\n",
        "name: aml-Pillow\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.8\n",
        "  - numpy=1.21.2\n",
        "  - pip=21.2.4\n",
        "  - scikit-learn=0.24.2\n",
        "  - scipy=1.7.1\n",
        "  - pandas>=1.1,<1.2\n",
        "  - pip:\n",
        "    - inference-schema[numpy-support]==1.3.0\n",
        "    - xlrd==2.0.1\n",
        "    - mlflow==1.26.1\n",
        "    - azureml-mlflow==1.42.0\n",
        "    - Pillow==10.0.1\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(directory_path, exist_ok=True)\n",
        "\n",
        "with open(file_path, \"w\") as f:\n",
        "    f.write(conda_yaml_content.strip()) \n",
        "\n",
        "print(f\"Successfully created '{file_path}' with the specified content.\")\n",
        "\n",
        "# verify reading back contents\n",
        "with open(file_path, \"r\") as f:\n",
        "    print(\"\\n--- Content of conda.yaml ---\")\n",
        "    print(f.read())\n",
        "    print(\"-----------------------------\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Successfully created 'components/dataprep/conda.yaml' with the specified content.\n\n--- Content of conda.yaml ---\nname: aml-Pillow\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.8\n  - numpy=1.21.2\n  - pip=21.2.4\n  - scikit-learn=0.24.2\n  - scipy=1.7.1\n  - pandas>=1.1,<1.2\n  - pip:\n    - inference-schema[numpy-support]==1.3.0\n    - xlrd==2.0.1\n    - mlflow==1.26.1\n    - azureml-mlflow==1.42.0\n    - Pillow==10.0.1\n-----------------------------\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1748626189586
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using magic"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p components/dataprep # -p is the same as 'create if not exists' \n",
        "\n",
        "# Use %%writefile to write the content directly to the file\n",
        "# this command must be the first line in the cell and there can't be any comments or it won't work\n",
        "# After executing this cell, the file 'components/dataprep/conda.yaml' will be created/overwritten."
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1748626189810
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile components/dataprep/conda.yaml \n",
        "name: aml-Pillow\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.8\n",
        "  - numpy=1.21.2\n",
        "  - pip=21.2.4\n",
        "  - scikit-learn=0.24.2\n",
        "  - scipy=1.7.1\n",
        "  - pandas>=1.1,<1.2\n",
        "  - pip:\n",
        "    - inference-schema[numpy-support]==1.3.0\n",
        "    - xlrd==2.0.1\n",
        "    - mlflow==1.26.1\n",
        "    - azureml-mlflow==1.42.0\n",
        "    - Pillow==10.0.1"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting components/dataprep/conda.yaml\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1748601789979
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the environment “aml-Pillow”. aml stands for “Azure Machine Learning” and Pillow  is a well known Image Processing library.\n",
        "from azure.ai.ml.entities import Environment\n",
        "import os\n",
        "\n",
        "custom_env_name = \"aml-Pillow\"\n",
        "\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for Image Processing (with Pillow)\",\n",
        "    tags={\"Pillow\": \"10.0.1\"},\n",
        "    conda_file=os.path.join(\"components\", \"dataprep\", \"conda.yaml\"),\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "    )\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Environment with name aml-Pillow is registered to workspace, the environment version is 1\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1748626192081
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## creating data prep component"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "# This registers a component with the name \"data_prep_image_resize\"\n",
        "# Which can then be used in the Pipeline editor of the Azure Portal\n",
        "data_prep_component = command(\n",
        "    name=\"data_prep_image_resize\",\n",
        "    display_name=\"Data preparation, Image Resizing\",\n",
        "    description=\"Reads a data asset of images and preprocesses them by resizing them to 64 to 64.\",\n",
        "    # Which input data will we receive? We will be splitting each batch of images individually for each animal type.\n",
        "    inputs={\n",
        "        \"data\": Input(type=\"uri_folder\"),\n",
        "    },\n",
        "    # We need the \"rw_mount\" (Read/Write mount) so that our code can also write to the output folder and save the images\n",
        "    outputs={\n",
        "        \"output_data\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "    },\n",
        "    # The source folder of the components code. It will upload all the files in that directory\n",
        "    code=os.path.join(\"components\", \"dataprep\"),\n",
        "    command=\"\"\"python dataprep.py \\\n",
        "            --data ${{inputs.data}} \\\n",
        "            --output_data ${{outputs.output_data}} \\\n",
        "            \"\"\",\n",
        "    environment=f\"aml-Pillow@latest\",\n",
        ")\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "data_prep_component = ml_client.create_or_update(data_prep_component.component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {data_prep_component.name} with Version {data_prep_component.version} is registered\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component data_prep_image_resize with Version 2025-05-30-17-29-52-6002454 is registered\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1748626195262
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### first pipeline component"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure ML pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=\"cpu-daymon-auto\",\n",
        "    description=\"Custom data_prep pipeline\",\n",
        ")\n",
        "def animal_images_preprocessing_pipeline(\n",
        "    input_version: str, # Currently we don't use these version numbers, but we will use them later on.\n",
        "    output_version: str,\n",
        "):\n",
        "    # using data_prep_function like a python call with its own inputs\n",
        "    # These are the animals with the version name as a second item in the tuple\n",
        "    animals = [\n",
        "        ('pandas', \"1\"),\n",
        "        ('cats', \"1\"),\n",
        "        ('dogs', \"1\")\n",
        "    ] # They are hardcoded in here, because we should give them from another component otherwise.\n",
        "    \n",
        "    jobs = {}\n",
        "    for animal in animals:\n",
        "\n",
        "        data_prep_job = data_prep_component(\n",
        "            data=Input(\n",
        "                type=\"uri_folder\",\n",
        "                path=f\"azureml:{animal[0]}:{animal[1]}\" \n",
        "            ),\n",
        "        )\n",
        "        \n",
        "        output_name = animal[0] + \"_resized\"\n",
        "        # Update the subscriptionID, resourcegroup and workspace name here as well\n",
        "        workspace_name = os.environ.get('WORKSPACE', 'pollet-daymon-ml')\n",
        "        subscription_id = os.environ.get('SUBSCRIPTION_ID', '6a36bb7a-aee5-4e15-a3c7-2e362d2c2387')\n",
        "        resource_group = os.environ.get('RESOURCE_GROUP', 'mlops-demo')\n",
        "        output_path = f\"azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace_name}/datastores/workspaceblobstore/paths/processed_animals/\" + animal[0]\n",
        "\n",
        "        data_prep_job.outputs.output_data = Output(\n",
        "            type=\"uri_folder\",\n",
        "            path=output_path,\n",
        "            name=output_name,\n",
        "            mode=\"rw_mount\"\n",
        "        )\n",
        "\n",
        "        jobs[animal[0]] = data_prep_job\n",
        "\n",
        "    # a pipeline returns a dictionary of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        k: v.outputs.output_data for k,v in jobs.items()\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1748626195649
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's instantiate the pipeline with the parameters of our choice\n",
        "pipeline = animal_images_preprocessing_pipeline()"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1748626195794
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import webbrowser\n",
        "\n",
        "# submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline,\n",
        "    # Project's name\n",
        "    experiment_name=\"image_preprocessing_pipeline\",\n",
        ")\n",
        "# open the pipeline in web browser\n",
        "webbrowser.open(pipeline_job.studio_url)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\npathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\npathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\npathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "False"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1748626198537
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "import os\n",
        "\n",
        "# Define the data split component\n",
        "data_split_component = command(\n",
        "    name=\"data_split\",\n",
        "    display_name=\"Dataset Train/Test Split\",\n",
        "    description=\"Splits input image datasets into training and testing sets based on a specified ratio.\",\n",
        "    # Define the inputs for the component\n",
        "    inputs={\n",
        "        \"animal_1\": Input(type=\"uri_folder\", description=\"Path to the first animal image dataset.\"),\n",
        "        \"animal_2\": Input(type=\"uri_folder\", description=\"Path to the second animal image dataset.\"),\n",
        "        \"animal_3\": Input(type=\"uri_folder\", description=\"Path to the third animal image dataset.\"),\n",
        "        \"train_test_split_factor\": Input(type=\"number\", description=\"Percentage of data to use for testing (e.g., 0.2 for 20%).\"),\n",
        "    },\n",
        "    # Define the outputs for the component\n",
        "    outputs={\n",
        "        \"training_data\": Output(type=\"uri_folder\", mode=\"rw_mount\", description=\"Output path for the training dataset.\"),\n",
        "        \"testing_data\": Output(type=\"uri_folder\", mode=\"rw_mount\", description=\"Output path for the testing dataset.\"),\n",
        "    },\n",
        "    # specify source code dir relative to where we are right now in this notebook\n",
        "    code=os.path.join(\"components\", \"dataprep\", \"code\"),\n",
        "    # mapping input and output to script arguments\n",
        "    command=\"\"\"python traintestsplit.py \\\n",
        "            --datasets ${{inputs.animal_1}} ${{inputs.animal_2}} ${{inputs.animal_3}} \\\n",
        "            --split_size ${{inputs.train_test_split_factor}} \\\n",
        "            --training_data_output ${{outputs.training_data}} \\\n",
        "            --testing_data_output ${{outputs.testing_data}} \\\n",
        "            \"\"\",\n",
        "    # command for running this \n",
        "    environment=f\"aml-Pillow@latest\",\n",
        ")\n",
        "\n",
        "# this is how to register a component to a workspace\n",
        "data_split_component = ml_client.create_or_update(data_split_component.component)\n",
        "\n",
        "# check by printing for comformation\n",
        "print(\n",
        "    f\"Component {data_split_component.name} with Version {data_split_component.version} is registered.\"\n",
        ")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component data_split with Version 2025-05-30-17-29-58-7844951 is registered.\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1748626201092
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "using the above component in a pipeline:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import dsl, Input, Output\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=\"cpu-daymon-auto\",\n",
        "    description=\"Custom data_split pipeline\",\n",
        ")\n",
        "def animal_images_traintest_split_pipeline(\n",
        "    train_test_split: int, # Currently we don't use these version numbers, but we will use them later on.\n",
        "    animal_1: Input,\n",
        "    animal_2: Input,\n",
        "    animal_3: Input,\n",
        "):\n",
        "    # using data_prep_function like a python call with its own inputs\n",
        "    # These are the animals with the version name as a second item in the tuple\n",
        "\n",
        "    # Combining arguments starting with \"animals_\" into a dictionary\n",
        "    animals_args = {k: v for k, v in locals().items() if k.startswith(\"animals_\")}\n",
        "\n",
        "    # Create a component instance by calling the component factory\n",
        "    data_split_job = data_split_component(\n",
        "            animal_1=animal_1,\n",
        "            animal_2=animal_2,\n",
        "            animal_3=animal_3,\n",
        "            train_test_split_factor=train_test_split\n",
        "        )\n",
        "    \n",
        "    # Override the training data output and testing data output to a file named \"trainingdata\" and \"testingdata\n",
        "    data_split_job.outputs.training_data = Output(\n",
        "        type=\"uri_folder\",\n",
        "        name=\"training_data\",\n",
        "        mode=\"rw_mount\"\n",
        "    )\n",
        "    data_split_job.outputs.testing_data = Output(\n",
        "        type=\"uri_folder\",\n",
        "        name=\"testing_data\",\n",
        "        mode=\"rw_mount\"\n",
        "    )\n",
        "\n",
        "    # a pipeline returns a dictionary of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        \"training_data\": data_split_job.outputs.training_data,\n",
        "        \"testing_data\": data_split_job.outputs.testing_data\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1748626201218
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from azure.ai.ml import Input \n",
        "\n",
        "version = \"1\" \n",
        "animals = [\"pandas\", \"cats\", \"dogs\"]\n",
        "\n",
        "animals_datasets = {\n",
        "    f\"animal_{i+1}\": Input(type=\"uri_folder\", path=f\"azureml:{animal}_resized:{version}\")\n",
        "    for i, animal in enumerate(animals)\n",
        "}\n",
        "\n",
        "split_percentage_for_pipeline = 20\n",
        "\n",
        "\n",
        "print(\"Creating pipeline job instance...\")\n",
        "pipeline_job = animal_images_traintest_split_pipeline(\n",
        "    train_test_split=split_percentage_for_pipeline,\n",
        "    **animals_datasets # **animal_dataset_inputs unpacks the dictionary into keyword arguments (we finally get to use it again)\n",
        ")\n",
        "\n",
        "pipeline_job.experiment_name = \"data_preparation_and_split_pipeline_run\"\n",
        "pipeline_job.display_name = \"Animal Image Train-Test Split Pipeline\"\n",
        "\n",
        "returned_pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline_job,\n",
        "    wait=True \n",
        ")\n",
        "\n",
        "print(f\"\\nPipeline Job submitted: {returned_pipeline_job.name}\")\n",
        "print(f\"Pipeline Job Status: {returned_pipeline_job.status}\")\n",
        "print(f\"Pipeline Job URL: {returned_pipeline_job.studio_url}\")\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\npathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nPipeline Job submitted: gifted_fennel_zpshq4757h\nPipeline Job Status: NotStarted\nPipeline Job URL: https://ml.azure.com/runs/gifted_fennel_zpshq4757h?wsid=/subscriptions/6a36bb7a-aee5-4e15-a3c7-2e362d2c2387/resourcegroups/mlops-demo/workspaces/pollet-daymon-ml&tid=4f3f75e5-d447-48c8-9483-c82b6c655896\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1748626203099
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "import os\n",
        "\n",
        "custom_env_name = \"aml-Tensorflow-Pillow\"\n",
        "\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for AI Training (with Pillow)\",\n",
        "    tags={\"Pillow\": \"10.0.1\", \"Tensorflow\": \"2.4.1\"},\n",
        "    conda_file=os.path.join(\"components\", \"training\", \"conda.yaml\"),\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        ")\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Environment with name aml-Tensorflow-Pillow is registered to workspace, the environment version is 2\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1748626209824
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "define the training component"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "training_component = command(\n",
        "    name=\"training\",\n",
        "    display_name=\"Training an AI model\",\n",
        "    description=\"Trains an AI model by inputting a lot of training and testing data with configurable hyperparameters.\",\n",
        "    inputs={\n",
        "        \"training_folder\": Input(type=\"uri_folder\"),\n",
        "        \"testing_folder\": Input(type=\"uri_folder\"),\n",
        "        \"epochs\": Input(type=\"integer\"), # Changed to integer as epochs are typically integers\n",
        "        # --- ADDED NEW DYNAMIC INPUTS HERE ---\n",
        "        \"seed\": Input(type=\"integer\", default=42, description=\"Random seed for reproducibility.\"),\n",
        "        \"initial_learning_rate\": Input(type=\"number\", default=0.001, description=\"Initial learning rate for optimizer.\"),\n",
        "        \"batch_size\": Input(type=\"integer\", default=32, description=\"Batch size for training.\"),\n",
        "        \"patience\": Input(type=\"integer\", default=5, description=\"Patience for early stopping.\"),\n",
        "        \"model_name\": Input(type=\"string\", default=\"animal-cnn\", description=\"Name for the saved AI model.\"),\n",
        "    },\n",
        "    outputs={\n",
        "        \"output_folder\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "    },\n",
        "    code=os.path.join(\"components\", \"training\", \"code\"),\n",
        "    command=\"\"\"python train.py \\\n",
        "            --training_folder ${{inputs.training_folder}} \\\n",
        "            --testing_folder ${{inputs.testing_folder}} \\\n",
        "            --output_folder ${{outputs.output_folder}} \\\n",
        "            --epochs ${{inputs.epochs}} \\\n",
        "            --seed ${{inputs.seed}} \\\n",
        "            --initial_learning_rate ${{inputs.initial_learning_rate}} \\\n",
        "            --batch_size ${{inputs.batch_size}} \\\n",
        "            --patience ${{inputs.patience}} \\\n",
        "            --model_name ${{inputs.model_name}} \\\n",
        "            \"\"\",\n",
        "    environment=f\"aml-Tensorflow-Pillow@latest\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1748626210031
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "register the training component"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_component = ml_client.create_or_update(training_component.component)\n",
        "\n",
        "print(\n",
        "    f\"Component {training_component.name} with Version {training_component.version} is registered.\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component training with Version 2025-05-30-17-30-10-0222014 is registered.\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1748626212174
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "create the pipeline for this component"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure ML pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=\"cpu-daymon-auto\",\n",
        "    description=\"Custom Animals Training pipeline\",\n",
        ")\n",
        "def animals_training_pipeline(\n",
        "    training_folder: Input, # Currently we don't use these version numbers, but we will use them later on.\n",
        "    testing_folder: Input,\n",
        "    epochs: int,\n",
        "):\n",
        "\n",
        "    training_job = training_component(\n",
        "        training_folder=training_folder,\n",
        "        testing_folder=testing_folder,\n",
        "        epochs=epochs\n",
        "    )\n",
        "    \n",
        "    # Let Azure decide a unique place everytime\n",
        "    training_job.outputs.output_folder = Output(\n",
        "        type=\"uri_folder\",\n",
        "        name=\"output_data\",\n",
        "        mode=\"rw_mount\"\n",
        "    )\n",
        "\n",
        "\n",
        "    # a pipeline returns a dictionary of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        \"output_data\": training_job.outputs.output_folder,\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1748626212307
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "instantiate the pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's instantiate the pipeline with the parameters of our choice\n",
        "\n",
        "# Make sure to use the correct version number here!\n",
        "training_pipeline = animals_training_pipeline(\n",
        "    # Change these versions if you want to override the choices\n",
        "    training_folder=Input(type=\"uri_folder\", path=f\"azureml:training_data:1\"),\n",
        "    testing_folder=Input(type=\"uri_folder\", path=f\"azureml:testing_data:1\"),\n",
        "    epochs=5 # This isn't super important now, the quality of the AI model isn't the most important, so training it longer will just waste resources anyways ...\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1748626212436
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "create the pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import webbrowser\n",
        "# submit the pipeline job\n",
        "training_pipeline_job = ml_client.jobs.create_or_update(\n",
        "    training_pipeline,\n",
        "    # Project's name\n",
        "    experiment_name=\"training_pipeline\",\n",
        ")\n",
        "# open the pipeline in web browser\n",
        "webbrowser.open(training_pipeline_job.studio_url)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": "False"
          },
          "metadata": {}
        }
      ],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1748626213977
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapping everything into one pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "\n",
        "\n",
        "# --- Define the Master End-to-End Pipeline ---\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=\"cpu-daymon-auto\", \n",
        "    description=\"Master End-to-End Animal Image Classification Pipeline - Chaining Components Directly\",\n",
        "    default_compute=\"cpu-daymon-auto\", \n",
        "    display_name=\"Master Animal ML Pipeline (Components Chained)\"\n",
        ")\n",
        "def master_animal_ml_pipeline_from_components(\n",
        "    pandas_raw_data_input: Input,\n",
        "    cats_raw_data_input: Input,\n",
        "    dogs_raw_data_input: Input,\n",
        "    \n",
        "    train_test_split_percentage: int = 20, \n",
        "    \n",
        "    epochs: int = 50, \n",
        "    seed: int = 42,\n",
        "    initial_learning_rate: float = 0.001,\n",
        "    batch_size: int = 32,\n",
        "    patience: int = 5,\n",
        "    final_model_name: str = \"master_animal_classifier_direct_chain\", \n",
        "):\n",
        "    # --- Step 1: Image Preprocessing (Resizing) using data_prep_component ---\n",
        "    pandas_resized_job = data_prep_component(data=pandas_raw_data_input)\n",
        "    cats_resized_job = data_prep_component(data=cats_raw_data_input)\n",
        "    dogs_resized_job = data_prep_component(data=dogs_raw_data_input)\n",
        "\n",
        "    # --- Step 2: Train/Test Split using data_split_component ---\n",
        "    split_job = data_split_component(\n",
        "        animal_1=pandas_resized_job.outputs.output_data, \n",
        "        animal_2=cats_resized_job.outputs.output_data,   \n",
        "        animal_3=dogs_resized_job.outputs.output_data,   \n",
        "        train_test_split_factor=train_test_split_percentage \n",
        "    )\n",
        "    split_job.outputs.training_data = Output(type=\"uri_folder\", name=\"combined_training_data\", mode=\"rw_mount\")\n",
        "    split_job.outputs.testing_data = Output(type=\"uri_folder\", name=\"combined_testing_data\", mode=\"rw_mount\")\n",
        "\n",
        "\n",
        "    # --- Step 3: Model Training using training_component ---\n",
        "\n",
        "    training_job = training_component(\n",
        "        training_folder=split_job.outputs.training_data, \n",
        "        testing_folder=split_job.outputs.testing_data,  \n",
        "        epochs=epochs,\n",
        "        seed=seed,\n",
        "        initial_learning_rate=initial_learning_rate,\n",
        "        batch_size=batch_size,\n",
        "        patience=patience,\n",
        "        model_name=final_model_name, \n",
        "    )\n",
        "\n",
        "    # to be used by the 'train.py' script for model registration.\n",
        "    training_job.outputs.output_folder = Output(\n",
        "        type=\"uri_folder\",\n",
        "        name=\"trained_model_artifacts\", # Use a static, descriptive name for the output folder\n",
        "        mode=\"rw_mount\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"final_model_output_asset\": training_job.outputs.output_folder, \n",
        "        \"final_training_data_asset\": split_job.outputs.training_data,\n",
        "        \"final_testing_data_asset\": split_job.outputs.testing_data,\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1748626214201
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_animal_inputs_for_master_pipeline = {\n",
        "    \"pandas_raw_data_input\": Input(type=\"uri_folder\", path=\"azureml:pandas:1\"),\n",
        "    \"cats_raw_data_input\": Input(type=\"uri_folder\", path=\"azureml:cats:1\"),\n",
        "    \"dogs_raw_data_input\": Input(type=\"uri_folder\", path=\"azureml:dogs:1\")\n",
        "}\n",
        "\n",
        "print(\"Creating master end-to-end pipeline job instance (chaining components directly)...\")\n",
        "master_components_pipeline_job_instance = master_animal_ml_pipeline_from_components(\n",
        "    **raw_animal_inputs_for_master_pipeline, \n",
        "    train_test_split_percentage=25, \n",
        "    epochs=75, \n",
        "    seed=12345, \n",
        "    initial_learning_rate=0.0001, \n",
        "    batch_size=128, \n",
        "    patience=15, \n",
        "    final_model_name=\"my_e2e_animal_classifier_direct_chain_v1\"\n",
        ")\n",
        "\n",
        "master_components_pipeline_job_instance.experiment_name = \"master_e2e_component_chaining\"\n",
        "master_components_pipeline_job_instance.display_name = \"Master Animal ML Pipeline (Direct Component Chain)\"\n",
        "\n",
        "print(f\"\\nSubmitting the master pipeline job '{master_components_pipeline_job_instance.display_name}'...\")\n",
        "returned_master_components_pipeline_job = ml_client.jobs.create_or_update(\n",
        "    master_components_pipeline_job_instance,\n",
        "    wait=True \n",
        ")\n",
        "\n",
        "print(f\"\\nMaster Chained Components Pipeline Job submitted: {returned_master_components_pipeline_job.name}\")\n",
        "print(f\"Master Chained Components Pipeline Job Status: {returned_master_components_pipeline_job.status}\")\n",
        "print(f\"Master Chained Components Pipeline Job URL: {returned_master_components_pipeline_job.studio_url}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Creating master end-to-end pipeline job instance (chaining components directly)...\n\nSubmitting the master pipeline job 'Master Animal ML Pipeline (Direct Component Chain)'...\n\nMaster Chained Components Pipeline Job submitted: boring_hamster_bw56rs21r6\nMaster Chained Components Pipeline Job Status: NotStarted\nMaster Chained Components Pipeline Job URL: https://ml.azure.com/runs/boring_hamster_bw56rs21r6?wsid=/subscriptions/6a36bb7a-aee5-4e15-a3c7-2e362d2c2387/resourcegroups/mlops-demo/workspaces/pollet-daymon-ml&tid=4f3f75e5-d447-48c8-9483-c82b6c655896\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\npathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\npathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
        }
      ],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1748626216275
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}